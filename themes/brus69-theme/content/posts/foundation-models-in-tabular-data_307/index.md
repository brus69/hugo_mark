---
title: "Готовы ли базовые модели для обработки Ваших табличных данных? | На пути к науке о данных"
date: "2025-11-19T22:53:35+0000"
draft: false
description: ""
h1: "Готовы ли базовые модели для Ваших производственных табличных данных?"
urldel: "https://towardsdatascience.com/foundation-models-in-tabular-data/"
---

### Обучающие модели для табличных данных

Крупномасштабные модели искусственного интеллекта обучаются на обширном и разнообразном наборе данных, таком как аудио, текст, изображения или их комбинация. Благодаря своей универсальности базовые модели совершают революцию в обработке естественного языка, компьютерном зрении и даже в анализе временных рядов. В отличие от традиционных алгоритмов искусственного интеллекта, базовые модели предлагают готовые прогнозы без необходимости обучения с нуля для каждого конкретного приложения. Их также можно адаптировать для более конкретных задач с помощью точной настройки.

В последние годы мы наблюдаем взрывной рост применения базовых моделей для неструктурированных данных и временных рядов. К ним относятся серии OpenAI GPT и BERT для текстовых задач, CLIP и SAM для обнаружения объектов, классификации и сегментации, а также PatchTST, Lag-Llama и Moirai-MoE для прогнозирования временных рядов. Несмотря на этот рост, базовые модели для табличных данных остаются в значительной степени неизученными из-за ряда проблем.

#### TabPFN

Давайте начнём с представления наиболее известной базовой модели для табличных данных малого и среднего размера: TabPFN. Этот алгоритм был разработан компанией Prior Labs. Первая версия была выпущена в 2022 году, но обновления к его архитектуре были выпущены в январе 2025 года.

TabPFN — это сеть, основанная на предшествующих данных, что означает, что она использует байесовский вывод для прогнозирования. Есть два важных понятия в байесовском выводе: предшествующее и апостериорное. Предшествующее — это распределение вероятностей, отражающее наши убеждения или предположения о параметрах до наблюдения каких-либо данных. Например, вероятность выпадения шестёрки при броске игральной кости равна _1/6_. Апостериорное — это обновлённое убеждение или распределение вероятностей после наблюдения данных. Оно объединяет ваши первоначальные предположения (предшествующее) с новыми доказательствами.

В TabPFN предшествующее определяется 100 миллионами синтетических наборов данных, которые были тщательно разработаны для охвата широкого спектра потенциальных сценариев, с которыми может столкнуться модель. Эти наборы данных содержат широкий спектр взаимосвязей между признаками и целями.

#### Модельная архитектура

Архитектура TabPFN показана на следующем рисунке.

Левая часть диаграммы показывает типичный табличный набор данных. Он состоит из нескольких обучающих строк со входными признаками (_x_ 1, _x_ 2) и соответствующими значениями целевых переменных (_y_). Также имеется одна тестовая строка, которая имеет входные признаки, но отсутствует значение целевой переменной. Цель сети — предсказать значение целевой переменной для этой тестовой строки.

Архитектура TabPFN состоит из серии из 12 идентичных слоёв. Каждый слой содержит два механизма внимания. Первый — это 1D-внимание к признакам, которое изучает взаимосвязи между признаками набора данных. Второй механизм внимания — это 1D-выборочное внимание. Этот модуль рассматривает один и тот же признак во всех других выборках.

Выход 12 слоёв представляет собой вектор, который подаётся в многослойный перцептрон (MLP). MLP — это небольшая нейронная сеть, которая преобразует вектор в окончательный прогноз. Для задачи классификации окончательный прогноз не является меткой класса. Вместо этого MLP выводит вектор вероятностей, где каждое значение представляет уверенность модели в том, что входные данные принадлежат к определённому классу.

Для регрессионных задач выходной слой MLP модифицирован так, чтобы выдавать непрерывное значение вместо вероятностного распределения по дискретным классам.

#### Использование

Использование TabPFN довольно просто! Вы можете установить его через pip или из исходного кода. Есть отличная документация, предоставленная Prior Labs, которая ссылается на различные репозитории GitHub, где вы можете найти записные книжки Colab для изучения этого алгоритма прямо сейчас. Python API похож на API Scikit Learn, использующий функции `fit/predict`.

Функция `fit` в TabPFN не означает, что модель будет обучена как в классическом подходе машинного обучения. Вместо этого функция `fit` использует обучающий набор данных в качестве контекста. Это связано с тем, что TabPFN использует ICL. В этом подходе модель использует свои существующие знания и обучающие выборки для понимания закономерностей и генерации более точных прогнозов.

#### CARTE

Вторая базовая модель для табличных данных использует графовые структуры для создания интересной архитектуры модели: я говорю о модели _Context Aware Representation of Table Entrie_ s, или CARTE.

В отличие от изображений, где объект имеет определённые признаки независимо от его внешнего вида на изображении, числа в табличных данных не имеют значения, если не добавлен контекст через соответствующие имена столбцов.

CARTE преобразует таблицу в графовую структуру, преобразуя каждую строку в графолет. Строка в наборе данных представлена в виде небольшого звездообразного графа, где каждое значение строки становится узлом, соединённым с центральным узлом. Имена столбцов служат рёбрами графа.

#### Модельная архитектура

Каждый из созданных графолетов содержит признаки узла (_X_) и ребра (_E_). Эти признаки передаются в графо-внимательную сеть, которая адаптирует классическую архитектуру кодера Transformer. Ключевым компонентом этой графо-внимательной сети является слой самовнимания, который вычисляет внимание как по признакам узла, так и по признакам ребра. Это позволяет модели понять контекст каждой записи данных.

Архитектура модели CARTE также включает слой Aggregate & Readout, который действует на центральный узел. Выходные данные обрабатываются для контрастной потери.

#### Использование и ограничения

Репозиторий GitHub для CARTE находится в стадии активной разработки. Он содержит записную книжку Colab с примерами использования этой модели для задач регрессии и классификации. Согласно этой записной книжке, установка довольно проста, просто через `pip install`. Как и TabPFN, CARTE использует интерфейс Scikit-learn (`fit-predict`) для прогнозирования на невидимых данных.

Согласно статье CARTE, этот алгоритм имеет некоторые существенные преимущества, такие как устойчивость к пропущенным значениям. Кроме того, сопоставление сущностей не требуется при использовании CARTE. Поскольку он использует LLM для встраивания строк и имён столбцов, этот алгоритм может обрабатывать сущности, которые могут выглядеть по-разному, например, «Londres» вместо «London».

#### TabuLa-8b

Третья базовая модель, которую мы рассмотрим, была построена путём тонкой настройки языковой модели Llama 3-8B. Согласно авторам TabuLa-8b, языковые модели можно обучить для выполнения задач табличного прогнозирования путём сериализации строк в виде текста, преобразования текста в токены, а затем использования той же функции потерь и методов оптимизации в языковом моделировании.

Архитектура TabuLa-8b имеет эффективную схему маскировки внимания, называемую схемой Row-Causal Tabular Masking (RCTM). Эта маскировка позволяет модели учитывать все предыдущие строки из той же таблицы в пакете, но не строки из других таблиц. Эта структура побуждает модель учиться на небольшом количестве примеров в таблице, что имеет решающее значение для обучения с несколькими примерами.

#### Использование и ограничения

Репозиторий rtfm содержит код TabuLa-8b. Здесь вы найдёте в папке Notebooks пример того, как делать вывод. Обратите внимание, что, в отличие от TabPFN или CARTE, TabuLa-8b не имеет интерфейса Scikit-learn. Если вы хотите делать прогнозы с нуля или дополнительно настроить существующую модель, вам необходимо запустить скрипты Python, разработанные авторами.

Согласно оригинальной статье, TabuLa-8b хорошо справляется с задачами прогнозирования с нуля. Однако использование этой модели на больших таблицах с большим количеством выборок или с большим количеством признаков, а также с длинными именами столбцов может быть ограничено, поскольку эта информация может быстро превысить контекстное окно LLM (модель Llama 3-8B имеет контекстное окно в 8000 токенов).

#### TabDPT

Последняя базовая модель, которую мы рассмотрим в этом блоге, — это табличный дискриминантный предварительно обученный трансформер, или TabDPT для краткости. Как и TabPFN, TabDPT сочетает ICL с самоконтролируемым обучением для создания мощной базовой модели для табличных данных. TabDPT обучен на реальных данных (авторы использовали 123 общедоступных набора данных из OpenML). Согласно авторам, модель может обобщаться на новые задачи без дополнительного обучения или настройки гиперпараметров.

#### Модельная архитектура

TabDPT использует строковый трансформер-энкодер, аналогичный TabPFN, где каждая строка служит токеном. Чтобы справиться с различным количеством признаков обучающих данных (_F_), авторы стандартизировали измерение признака _F_ max с помощью дополнения (_F_ < _F_ max) или уменьшения размерности (_F_ > _F_ max).

Эта базовая модель использует самоконтролируемое обучение, по сути, обучаясь самостоятельно, без необходимости помеченного целевого значения для каждой задачи. Во время обучения он случайным образом выбирает один столбец в таблице, чтобы сделать его целевым, а затем учится предсказывать его значения на основе других столбцов.

#### Использование и ограничения

Существует репозиторий GitHub, который предоставляет код для генерации прогнозов на новых наборах табличных данных. Как и TabPFN или CARTE, TabDPT использует API, аналогичный Scikit-learn, для прогнозирования на невидимых данных, где функция `fit` использует обучающие данные для использования ICL. Код этой модели в настоящее время находится в стадии активной разработки.

Согласно авторам, модель имеет предопределённое максимальное количество признаков и классов. Авторы предлагают использовать анализ главных компонент (PCA) для уменьшения количества признаков, если таблица превышает лимит. Для задач классификации с большим количеством классов, чем у модели, проблему можно разбить на несколько подзадач, представив номер класса в другой системе счисления.

#### Основные выводы

В этом блоге я кратко описал базовые модели для табличных данных. Большинство из них были выпущены в 2024 году, но все они находятся в стадии активной разработки. Несмотря на то, что они довольно новы, некоторые из этих моделей уже имеют хорошую документацию и простоту использования. Например, вы можете установить TabPFN, CARTE или TabDPT через pip. Кроме того, эти модели используют тот же вызов API, что и Scikit-learn, что упрощает их интеграцию в существующие приложения машинного обучения.

Согласно авторам представленных здесь базовых моделей, эти алгоритмы превосходят классические методы бустинга, такие как XGBoost или CatBoost. Однако базовые модели всё ещё не могут использоваться на больших наборах табличных данных, что ограничивает их использование, особенно в производственных средах. Это означает, что классический подход обучения модели машинного обучения для каждого набора данных по-прежнему является лучшим способом создания прогнозных моделей на основе табличных данных.

Большие успехи были достигнуты на пути к созданию базовой модели для табличных данных. Давайте посмотрим, что ждёт эту захватывающую область исследований в будущем!

Спасибо за чтение!

_Я — Кармен Мартинес Барбоса, учёный по данным, который любит делиться новыми алгоритмами, полезными для сообщества. Читайте мой контент на Medium или TDS._

#### Ссылки

[1] N. Hollman et al., TabPFN: A transformer that solves small tabular classification problems in a second (2023), table representation learning workshop.

[2] N. Hollman et al., Accurate predictions on small data with a tabular foundation model (2025), Nature.

[3] M.J. Kim, L Grinsztajn, and G. Varoquaux. CARTE: Pretaining and Transfer for Tabular Learning (2024), Proceedings of the 41st International conference on Machine Learning, Vienna, Austria.

[4] F. Mahdisoltani, J. Biega, and F.M. Suchanek. YAGO3: A knowledge base from multilingual wikipedias (2013), in CIDR.

[5] J. Gardner, J.C. Perdomo, L. Schmidt. Large Scale Transfer Learning for Tabular Data via Language Modeling (2025), NeurlPS.

[6] M. Junwei et al. TabDPT: Scaling Tabular Foundation Models on Real Data (2024), arXiv preprint, arXiv:2410.18164.

