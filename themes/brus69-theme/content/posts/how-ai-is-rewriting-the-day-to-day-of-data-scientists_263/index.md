---
title: "Как искусственный интеллект меняет повседневную жизнь специалистов по обработке данных | На пути к науке о данных"
date: "2025-11-19T23:13:09+0000"
draft: true
description: ""
h1: "Как искусственный интеллект меняет повседневную жизнь специалистов по обработке данных"
urldel: "https://towardsdatascience.com/how-ai-is-rewriting-the-day-to-day-of-data-scientists/"
---

### Статьи

Я изучил и сравнил множество инструментов искусственного интеллекта, например:
* агент Google Data Science Agent;
* ChatGPT, Claude, Gemini для Data Science;
* DeepSeek V3 и другие.

Однако это лишь малая часть всех доступных инструментов искусственного интеллекта для работы с данными.

Вот некоторые из тех, что я использовал на работе:
* **OpenAI API**: я использую его для категоризации и обобщения отзывов клиентов и выявления болевых точек продуктов (см. мою статью с инструкциями).
* **ChatGPT** и **Gemini**: они помогают мне составлять сообщения в Slack и электронные письма, писать аналитические отчёты и даже отзывы о производительности.
* **Glean AI**: я использовал Glean AI для быстрого поиска ответов во внутренней документации и коммуникациях.
* **Cursor** и **Copilot**: мне нравится просто нажимать Tab-Tab для автозаполнения кода и комментариев.
* **Hex Magic**: я использую Hex для совместных работы с блокнотами данных на работе. Они также предлагают функцию Hex Magic для написания кода и исправления ошибок с помощью диалогового искусственного интеллекта.
* **Snowflake Cortex**: Cortex AI позволяет пользователям вызывать конечные точки LLM, создавать сервисы RAG и преобразовывать текст в SQL, используя данные в Snowflake.

Я уверен, что вы можете добавить в этот список гораздо больше, и каждый день запускаются новые инструменты искусственного интеллекта. На данный момент практически невозможно составить полный список.

Поэтому в этой статье я хочу сделать шаг назад и сосредоточиться на более важном вопросе: **что действительно нужно специалистам по работе с данными и как им может помочь искусственный интеллект**.

В следующем разделе я сосредоточусь на двух основных направлениях — устранении низкоценных задач и ускорении высокоценной работы.

## 1. Устранение низкоценных задач

Я стал специалистом по данным, потому что искренне люблю находить бизнес-инсайты в сложных данных и принимать бизнес-решения. Однако, проработав в отрасли более семи лет, я должен признать, что не вся работа так увлекательна, как я надеялся.

Перед проведением углублённого анализа или построением моделей машинного обучения ежедневно приходится выполнять множество низкоценных задач — и во многих случаях это происходит потому, что у нас нет подходящих инструментов, позволяющих нашим заинтересованным сторонам самостоятельно заниматься аналитикой.

**Текущее состояние: мы работаем как интерпретаторы данных и привратники (иногда «SQL-обезьяны»)**

* Простые запросы на получение данных поступают ко мне и моей команде в Slack каждую неделю: «Каким был GMV в прошлом месяце?», «Можете ли вы вытащить список клиентов, соответствующих этим критериям?», «Можете ли вы помочь мне заполнить это число в колоде, которое мне нужно представить завтра?»
* **Инструменты бизнес-аналитики (BI) не всегда хорошо поддерживают сценарии самообслуживания**. Мы внедрили такие инструменты, как Looker и Tableau, чтобы заинтересованные стороны могли легко исследовать данные и отслеживать показатели. Но реальность такова, что всегда есть компромисс между простотой и возможностью самостоятельного использования. Иногда мы делаем информационные панели простыми для понимания с помощью нескольких показателей, но они могут выполнять лишь несколько задач. Между тем, если мы делаем инструмент очень настраиваемым, заинтересованные стороны могут посчитать его запутанным и неуверенно использовать его, а в худшем случае данные будут извлечены и интерпретированы неправильно.
* **Документация скудная или устаревшая**. Это обычная ситуация, но она может быть вызвана разными причинами — возможно, мы работаем быстро и фокусируемся на достижении результатов, или у нас нет хорошей политики управления данными и документации. В результате узконаправленные знания становятся узким местом для людей вне команды данных, чтобы использовать данные.

**Идеальное состояние: расширение прав и возможностей заинтересованных сторон для самостоятельного обслуживания, чтобы мы могли минимизировать низкоценную работу**

* Заинтересованные стороны могут легко и уверенно выполнять простые запросы на получение данных и отвечать на базовые вопросы о данных.
* Команды данных тратят меньше времени на повторяющуюся отчётность или одноразовые базовые запросы.
* Информационные панели можно найти, интерпретировать и использовать для действий без посторонней помощи.

Чтобы приблизиться к идеальному состоянию, какую роль здесь может сыграть искусственный интеллект?

1. **Запрос данных с помощью естественного языка (Text-to-SQL)**: один из способов снизить технический барьер — это позволить заинтересованным сторонам запрашивать данные с помощью естественного языка. В отрасли существует множество усилий по созданию Text-to-SQL:
    * Например, **Snowflake** — это компания, которая добилась больших успехов в области моделей Text2SQL и начала интегрировать эту возможность в свой продукт.
    * Многие компании (включая мою) также исследовали собственные решения Text2SQL. Например, **Uber** поделился своим опытом использования Uber’s QueryGPT, чтобы сделать запрос данных более доступным для своей команды операций. В этой статье подробно объясняется, как Uber разработал мультиагентскую архитектуру для генерации запросов. Между тем, были выявлены основные проблемы в этой области, включая точную интерпретацию намерений пользователя, обработку больших схем таблиц и предотвращение галлюцинаций и т. д.
    * Честно говоря, чтобы заставить Text-to-SQL работать, нужно преодолеть очень высокий порог, поскольку необходимо сделать запрос точным — даже если инструмент потерпит неудачу хотя бы один раз, это может разрушить доверие, и в конечном итоге заинтересованные стороны вернутся к вам для проверки запросов (тогда вам нужно будет прочитать + переписать запросы, что почти удваивает работу). Пока я не нашёл модель или инструмент Text-to-SQL, который работал бы идеально. Я вижу это достижимым, только когда вы запрашиваете данные из очень небольшого подмножества хорошо документированных основных наборов данных для конкретных и стандартизированных случаев использования, но очень сложно масштабировать до всех доступных данных и различных бизнес-сценариев.
    * Но, конечно, учитывая большой объём инвестиций в этой области и быстрое развитие искусственного интеллекта, я уверен, что мы будем всё ближе и ближе к точным и масштабируемым решениям Text-to-SQL.

2. **Чат-бот для бизнес-аналитики (BI assistant)**: ещё одна общая область для улучшения взаимодействия заинтересованных сторон с инструментами BI — это чат-бот для бизнес-аналитики. Это на самом деле делает шаг вперёд по сравнению с Text-to-SQL — вместо генерации SQL-запроса на основе подсказки пользователя он отвечает в формате визуализации плюс текстовое резюме.
    * **Gemini в Looker** является примером здесь. Looker принадлежит Google, поэтому для них очень естественно интегрироваться с Gemini. Ещё одно преимущество для Looker — это возможность создавать собственные функции искусственного интеллекта для аналитической аналитики в режиме реального времени. Хотя на основе моего ограниченного опыта работы с инструментом он часто выдаёт ошибки и иногда не может ответить на простые вопросы.
    * Tableau также запустила аналогичную функцию — **Tableau AI**. Я сам её не использовал, но, судя по демо, она помогает команде данных быстро готовить данные и создавать информационные панели, используя естественный язык, а также обобщать данные в «Tableau Pulse», чтобы заинтересованные стороны могли легко отслеживать изменения показателей и аномальные тенденции.

3. **Инструменты каталога данных**: искусственный интеллект также может помочь решить проблему скудной или устаревшей документации данных.
    * Во время одного внутреннего хакатона я помню, как один из проектов наших инженеров данных заключался в использовании LLM для увеличения охвата документации таблиц. ИИ способен считывать кодовую базу и с высокой точностью описывать столбцы, поэтому он может быстро улучшить документацию с минимальной проверкой и корректировкой со стороны человека.
    * Аналогично, когда моя команда создаёт новые таблицы, мы начали просить Cursor писать файлы документации YAML для таблиц, чтобы сэкономить время.
    * Существует множество инструментов каталога данных и инструментов управления, которые были интегрированы с искусственным интеллектом. Когда я ищу в Google «ai data catalog», я вижу логотипы инструментов каталога данных, таких как Atlan, Alation, Collibra, Informatica и т. д. (дисклеймер: я не использовал ни один из них). Это явно тенденция в отрасли.

## 2. Ускорение высокоценной работы

Теперь, когда мы поговорили о том, как искусственный интеллект может помочь с устранением низкоценных задач, давайте обсудим, как он может ускорить высокоценные проекты с данными. Здесь под высокоценной работой понимаются проекты с данными, которые сочетают в себе техническое совершенство с бизнес-контекстом и оказывают значимое влияние благодаря межфункциональному сотрудничеству.

**Текущее состояние: узкие места производительности существуют в повседневных рабочих процессах**

* **EDA занимает много времени**. Этот шаг имеет решающее значение для первоначального понимания данных, но может потребоваться много времени для проведения всех видов одномерного и многомерного анализа.
* **Время теряется на кодирование и отладку**. Давайте будем честными — никто не может помнить весь синтаксис numpy и pandas и параметры модели sklearn. Нам постоянно нужно искать документацию во время кодирования.
* **Богатые неструктурированные данные используются не полностью**. Бизнес ежедневно генерирует много текстовых данных из опросов, заявок в службу поддержки и обзоров. Но как масштабируемо извлечь из них инсайты, остаётся проблемой.

**Идеальное состояние: специалисты по данным сосредотачиваются на глубоком мышлении, а не на синтаксисе**

* Написание кода происходит быстрее без перерывов на поиск синтаксиса.
* Аналитики тратят больше времени на интерпретацию результатов и меньше времени на обработку данных.
* Неструктурированные данные больше не являются препятствием и могут быть быстро проанализированы.

### Заключение

Легко увлечься погоней за новейшими инструментами искусственного интеллекта. Но в конце концов, самое важное — это использовать искусственный интеллект для устранения того, что нас замедляет, и ускорения того, что продвигает нас вперёд. Ключ в том, чтобы оставаться прагматичным: внедрять то, что работает сегодня, сохранять любопытство к тому, что появляется, и никогда не упускать из виду основную цель науки о данных — принимать более взвешенные решения благодаря лучшему пониманию.

