---
title: "4 Метода оптимизации запросов LLM с точки зрения затрат, задержки и производительности | На пути к науке о данных"
date: "2025-11-23T23:13:09+0000"
draft: true
description: ""
h1: "4 Метода оптимизации запросов LLM с точки зрения затрат, задержки и производительности"
urldel: "https://towardsdatascience.com/4-techniques-to-optimize-your-llm-prompts-for-cost-latency-and-performance/"
---

### Оптимизация запросов к большим языковым моделям (LLM)

С момента выхода ChatGPT в 2022 году на рынке появилось множество продуктов на базе искусственного интеллекта, использующих большие языковые модели. Однако есть ещё много улучшений, которые следует внести в то, как мы используем LLM.

В этой статье я расскажу о нескольких конкретных методах, которые вы можете применить к способу создания и структурирования ваших запросов, что сократит задержки и затраты, а также повысит качество ответов. Цель — представить вам эти конкретные методы, чтобы вы могли сразу же внедрить их в своё приложение на базе LLM.

#### Почему стоит оптимизировать ваш запрос

Во многих случаях у вас может быть запрос, который работает с заданной LLM и даёт адекватные результаты. Однако вы не потратили много времени на оптимизацию запроса, что оставляет нереализованным большой потенциал.

Я утверждаю, что, используя конкретные методы, представленные в этой статье, вы можете легко улучшить качество своих ответов и сократить расходы без особых усилий. То, что запрос и LLM работают, не означает, что они работают оптимально, и во многих случаях вы можете добиться значительных улучшений, приложив совсем немного усилий.

#### Конкретные методы оптимизации

**1. Всегда сохраняйте статический контент в начале**

Первая техника, которую я рассмотрю, — это всегда сохранять статический контент в начале вашего запроса. Под статическим контентом я подразумеваю контент, который остаётся неизменным при выполнении нескольких вызовов API.

Причина, по которой вы должны сохранять статический контент в начале, заключается в том, что все крупные поставщики LLM, такие как Anthropic, Google и OpenAI, используют кэшированные токены. Кэшированные токены — это токены, которые уже были обработаны в предыдущем запросе API и могут быть обработаны дёшево и быстро.

Кэшированные токены обычно стоят около 10% от стоимости обычных входных токенов. Это означает, что если вы отправите один и тот же запрос два раза подряд, входные токены второго запроса будут стоить только 1/10 от стоимости входных токенов первого запроса. Это работает, потому что поставщики LLM кэшируют обработку этих входных токенов, что делает обработку вашего нового запроса дешевле и быстрее.

На практике кэширование входных токенов осуществляется путём сохранения переменных в конце запроса. Например, если у вас есть длинный системный запрос с вопросом, который меняется от запроса к запросу, вы должны сделать что-то вроде этого:

```
prompt = f"""
{long static system prompt}

{user prompt}
"""
```

Например:

```
prompt = f"""
You are a document expert ...
You should always reply in this format ...
If a user asks about ... you should answer ...

{user question}
"""
```

Здесь у нас сначала статический контент запроса, а затем переменный контент (вопрос пользователя) в конце.

**2. Вопрос в конце**

Ещё один метод, который вы должны использовать для повышения производительности LLM, — это всегда ставить вопрос пользователя в конце вашего запроса. В идеале вы должны организовать его так, чтобы ваш системный запрос содержал все общие инструкции, а пользовательский запрос просто состоял только из вопроса пользователя, например:

```
system_prompt = "<general instructions>"

user_prompt = f"{user_question}"
```

В документах по разработке подсказок в Anthropic утверждается, что включение пользовательского запроса в конце может повысить производительность до 30%, особенно если вы используете длинные контексты. Включение вопроса в конце делает модель более понятной в том, какую задачу она пытается выполнить, и во многих случаях приводит к лучшим результатам.

**3. Использование оптимизатора подсказок**

Часто, когда люди пишут запросы, они становятся беспорядочными, непоследовательными, включают избыточный контент и не имеют структуры. Поэтому вы всегда должны отправлять свой запрос через оптимизатор запросов.

Самый простой оптимизатор запросов, который вы можете использовать, — это попросить LLM _улучшить этот запрос {prompt}_, и он предоставит вам более структурированный запрос с меньшим количеством избыточного контента и так далее.

Однако ещё лучший подход — использовать специальный оптимизатор запросов, например, тот, который вы можете найти в консолях OpenAI или Anthropic. Эти оптимизаторы — это LLM, специально запрограммированные и созданные для оптимизации ваших запросов, и обычно они дают лучшие результаты.

**4. Бенчмарк LLM**

LLM, которую вы используете, также существенно повлияет на производительность вашего приложения на базе LLM. Разные LLM хороши для разных задач, поэтому вам нужно попробовать разные LLM в вашей конкретной области применения. Я рекомендую по крайней мере получить доступ к крупнейшим поставщикам LLM, таким как Google Gemini, OpenAI и Anthropic.

Вам также необходимо настроить специальный бенчмарк для задачи, которую вы пытаетесь выполнить, и посмотреть, какая LLM работает лучше всего. Кроме того, вы должны регулярно проверять производительность модели, поскольку крупные поставщики LLM периодически обновляют свои модели, не обязательно выпуская новую версию.

### Заключение

В этой статье я рассмотрел четыре различных метода, которые вы можете использовать для повышения производительности вашего приложения на базе LLM. Я обсудил использование кэшированных токенов, размещение вопроса в конце запроса, использование оптимизаторов запросов и создание специальных бенчмарков LLM. Все они относительно просты в настройке и могут привести к значительному повышению производительности. Я считаю, что существует множество подобных и простых методов, и вы всегда должны быть в поиске их.

