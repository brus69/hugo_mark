---
title: "Создание мультимодального RAG, который использует текст, изображения и таблицы из исходных текстов | На пути к науке о данных"
date: "2025-11-19T22:53:35+0000"
draft: false
description: ""
h1: "Создание мультимодального RAG, который будет дополняться текстом, изображениями и таблицами из исходных текстов"
urldel: "https://towardsdatascience.com/building-a-multimodal-rag-with-text-images-tables-from-sources-in-response/"
---

**Генерация (RAG)** стала одним из самых ранних и успешных применений генеративного ИИ. Однако немногие чат-боты возвращают _изображения, таблицы и графики_ из исходных документов наряду с текстовыми ответами.

В этом посте я исследую, почему сложно создать надёжную, действительно мультимодальную систему RAG, особенно для _сложных документов_, таких как научные статьи и корпоративные отчёты, которые часто содержат плотный текст, формулы, таблицы и графики.

Также здесь я представляю подход для **улучшенного мультимодального конвейера RAG**, который обеспечивает согласованные, высококачественные мультимодальные результаты для этих типов документов.

## Набор данных и настройка

Для иллюстрации я создал небольшую мультимодальную базу знаний, используя следующие документы:

1. _Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners_
2. _VectorPainter: Advanced Stylized Vector Graphics Synthesis Using Stroke-Style Priors_
3. _Marketing Strategy for Financial Services: Financing Farming & Processing the Cassava, Maize and Plantain Value Chains in Côte d’Ivoire_

Используемая языковая модель — **GPT-4o**, а для встраивания я использовал **text-embedding-3-small**.

## Стандартная мультимодальная архитектура RAG

Теоретически мультимодальный бот RAG должен:

* Принимать **текстовые и графические** запросы.
* Возвращать **текстовые и графические** ответы.
* Извлекать **контекст** как из текстовых, так и из графических источников.

Типичный конвейер выглядит следующим образом:

**1. Приём данных**

* **Парсинг и разбиение на фрагменты:** разделить документы на текстовые сегменты и извлечь изображения.
* **Суммирование изображений:** использовать LLM для генерации подписей или сводок для каждого изображения.
* **Мультивекторные вложения:** создать вложения для текстовых фрагментов, сводок изображений и, при необходимости, для необработанных характеристик изображений (например, используя CLIP).

**2. Индексация**

* Сохранить вложения и метаданные в векторной базе данных.

**3. Извлечение**

* Для пользовательского запроса выполнить поиск по сходству:
* Вложения текста (для текстовых совпадений)
* Вложения сводок изображений (для релевантности изображений)

**4. Генерация**

* Использовать мультимодальную LLM для синтеза окончательного ответа, используя как извлечённый текст, так и изображения.

## Внутреннее предположение

Этот подход предполагает, что **подпись или сводка изображения, сгенерированная из его содержимого, всегда содержит достаточно контекста** о тексте или темах, которые появляются в документе, для которых это изображение было бы подходящим ответом.

В реальных документах это часто не так.

**Пример: потеря контекста в корпоративных отчётах**

Возьмём отчёт «Marketing Strategy for Financial Services» (#3 в наборе данных) в наборе данных. В его кратком содержании есть две похожие таблицы, показывающие требования к _рабочему капиталу_ — одна для **основных производителей (фермеров)**, а другая для **переработчиков**.

GPT-4o генерирует следующее для первой таблицы:

_«Таблица описывает различные типы вариантов финансирования рабочего капитала для сельскохозяйственных предприятий, включая их цели и доступность в различных ситуациях»_

И следующее для второй таблицы:

_«Таблица содержит обзор вариантов финансирования рабочего капитала, детализируя их цели и потенциальную применимость в различных сценариях для предприятий, особенно для экспортёров и покупателей запасов»_

Оба варианта хороши по отдельности, но ни один не отражает **контекст**, который отличает _производителей_ от _переработчиков_.

Это означает, что они будут **извлечены неправильно** для запросов, специально касающихся только производителей или переработчиков. В других таблицах, таких как CAPEX, возможности финансирования, можно увидеть ту же проблему.

Для статьи о VectorPainter, где на рисунке 3 в статье показан конвейер VectorPainter, GPT-4o генерирует подпись: _«Обзор предлагаемой структуры для извлечения стиля на основе штрихов и стилизованного синтеза SVG со ограничениями на уровне штрихов»,_ упуская тот факт, что она представляет основную тему статьи, названную авторами «VectorPainter».

А для формулы потери дистилляции визуального языка, определённой в разделе 3.3 статьи о тонкой настройке CLIP, сгенерированная подпись: _«Уравнение, представляющее вариационное распределение логарифмов (VLD) потерь, определённое как сумма дивергенций Кульбака – Лейблера (KL) между прогнозируемыми и целевыми распределениями логарифмов в пакете входных данных»,_ где отсутствует контекст корреляции зрения и языка.

Также следует отметить, что в исследовательских статьях рисунки и таблицы имеют предоставленную автором подпись, однако в процессе извлечения она извлекается не как часть изображения, а как часть текста. А позиционирование подписи иногда бывает над фигурой, а иногда под ней. Что касается отчётов о маркетинговой стратегии, встроенные таблицы и другие изображения не имеют описания рисунка.

Выше было показано, что документы в реальном мире не следуют какому-либо стандартному формату текста, изображений, таблиц и подписей, что затрудняет процесс связывания контекста с фигурами.

## Новый и улучшенный мультимодальный конвейер RAG

Чтобы решить эту проблему, я внёс два ключевых изменения.

**1. Контекстно-зависимые сводки изображений**

Вместо того чтобы просить LLM суммировать изображение, я извлекаю **текст непосредственно перед и после рисунка** — до 200 символов в каждом направлении.

Таким образом, подпись к изображению включает:

* Предоставленную автором подпись (если есть)
* _Повествование_, которое придаёт ему смысл

Даже если в документе нет формальной подписи, это обеспечивает контекстуально точную сводку.

**2. Выбор изображений, управляемый текстовым ответом, во время генерации**

Во время извлечения я **не** сопоставляю пользовательский запрос напрямую с подписями к изображениям. Это связано с тем, что пользовательский запрос часто слишком короткий, чтобы предоставить достаточный контекст для извлечения изображений (например, «Что такое …?»).

Вместо этого:

* Сначала генерируется **текстовый ответ** с использованием верхних текстовых фрагментов, извлечённых для контекста.
* Затем **выбираются два лучших изображения** для текстового ответа, соответствующие подписям к изображениям.

Это обеспечивает выбор окончательных изображений **в связи с фактическим ответом**, а не только с запросом.

## Детали реализации

**Шаг 1: Извлечь текст и изображения**

Используйте Adobe PDF Extract API для анализа PDF-файлов в:

* папки figures/ и tables/ с файлами .png
* файл structuredData.json, содержащий позиции, текст и пути к файлам

Я обнаружил, что этот API гораздо надёжнее, чем такие библиотеки, как PyMuPDF, особенно для извлечения формул и диаграмм.

**Шаг 2: Создать текстовый файл**

Объединить все текстовые элементы из JSON для создания необработанного текстового корпуса:

```
elements = data.get("elements", [])
all_text = []
for el in elements:
  if "Text" in el:
    all_text.append(el["Text"].strip())
    final_text = "\n".join(all_text)
```

**Шаг 3: Создать подписи к изображениям**

Пройдитесь по каждому элементу `structuredData.json`, проверьте, заканчивается ли путь к файлу на `.png`. Загрузите файл из папок figures и tables документа, затем используйте LLM для проверки качества изображения. Это необходимо, поскольку процесс извлечения найдёт некоторые неразборчивые, маленькие изображения, заголовки и футеры, логотипы компаний и т. д., которые необходимо исключить из любых пользовательских ответов.

**Шаг 4: Разбить текст на фрагменты и создать вложения**

Текстовый файл документа разбивается на фрагменты по 1000 символов с помощью _`RecursiveCharacterTextSplitter`_ из `_langchain_ ` и сохраняется. Вложения создаются для текстовых фрагментов и подписей к изображениям, нормализуются и сохраняются в виде индексов `_faiss_ `.

**Шаг 5: Извлечение контекста и генерация ответа**

Пользовательский запрос сопоставляется, и извлекаются верхние 5 текстовых фрагментов в качестве контекста. Затем мы используем эти извлечённые фрагменты и пользовательский запрос, чтобы получить текстовый ответ с помощью LLM.

На следующем шаге мы берём сгенерированный текстовый ответ и находим два лучших совпадения изображений (на основе вложений подписей к изображениям) с ответом. Это отличается от традиционного способа сопоставления пользовательского запроса с вложениями изображений и даёт гораздо лучшие результаты.

## Тестовые результаты

Давайте запустим запросы, упомянутые в начале этого блога, чтобы увидеть, соответствуют ли извлечённые изображения релевантности пользовательскому запросу. Для простоты я печатаю только изображения и их подписи, отображаемые, а не текстовый ответ.

**Запрос 1:** _Каковы требования к кредитам и рабочему капиталу основного производителя?_

Рисунок 1: Обзор вариантов финансирования рабочего капитала для мелких, средних и крупных фермеров.

Рисунок 2: Варианты финансирования капитальных затрат для средних и крупных фермеров.

**Запрос 2:** _Каковы требования к кредитам и рабочему капиталу для переработчиков?_

Рисунок 1: Обзор кредитных продуктов для рабочего капитала для мелких и средних переработчиков.

Рисунок 2: Продукты CAPEX для покупки оборудования и расширения бизнеса на уровне переработки.

**Запрос 3:** _Что такое дистилляция зрения и языка?_

Рисунок 1: Формула потерь при дистилляции визуального языка для передачи модальной согласованности от предварительно обученного CLIP к тонко настроенным моделям.

Рисунок 2: Конечная целевая функция, объединяющая потери дистилляции, контролируемую контрастную потерю и потери дистилляции визуального языка с балансирующими гиперпараметрами.

**Запрос 4:** _Что такое конвейер VectorPainter?_

Рисунок 1: Обзор процесса извлечения стиля на основе штрихов и синтеза SVG, подчёркивающий векторизацию штрихов, потерю сохранения стиля и генерацию на основе текстовых подсказок.

Рисунок 2: Сравнение различных методов передачи стиля между растровыми и векторными форматами, демонстрирующее эффективность предлагаемого подхода в поддержании стилистической согласованности.

## Заключение

Этот усовершенствованный конвейер демонстрирует, как **контекстно-зависимое суммирование изображений** и **выбор изображений на основе текстового ответа** могут значительно повысить точность мультимодального извлечения.

Подход позволяет получать **насыщенные мультимодальные ответы**, сочетающие текст и визуальные элементы согласованным образом, что необходимо для исследовательских помощников, систем интеллектуального анализа документов и ботов с искусственным интеллектом, работающих со знаниями.

Попробуйте… оставляйте свои комментарии и свяжитесь со мной на www.linkedin.com/in/partha-sarkar-lets-talk-AI.

## Ресурсы

1. _Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners_: Mushui Liu, Bozheng Li, Yunlong Yu Zhejiang University
2. _VectorPainter: Advanced Stylized Vector Graphics Synthesis Using Stroke-Style Priors_: Juncheng Hu, Ximing Xing, Jing Zhang, Qian Yu† Beihang University
3. _Marketing Strategy for Financial Services: Financing Farming & Processing the Cassava, Maize and Plantain Value Chains in Côte d’Ivoire_ from https://www.ifc.org

