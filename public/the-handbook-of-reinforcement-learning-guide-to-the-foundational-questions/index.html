<!DOCTYPE html>
<html lang="ru" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>  
    Руководство по обучению с подкреплением: Руководство по основополагающим вопросам | На пути к науке о данных
     | P9X
  </title>
  <meta name="description" content="">
<meta name="yandex-verification" content="242f5a0ee52d5836" />
<meta name="google-site-verification" content="0Ytq4iozDTscUw8IMVUsSsLQ4yEXHRwKZSvFYqnSNtE" />
<link rel="canonical" href="http://localhost:1313/the-handbook-of-reinforcement-learning-guide-to-the-foundational-questions/" />

<link rel="icon" type="image/x-icon" href="http://localhost:1313/favicon.ico">

  
 <style>
  .content h1 { 
        font-size: 1.875rem; 
        font-weight: bold; 
        margin-bottom: 1rem; 
    }
    .content h2 { 
        font-size: 1.5rem; 
        font-weight: 600; 
        margin-bottom: 0.75rem; 
    }
    .content h3 { 
        font-size: 1.25rem; 
        font-weight: 500; 
        margin-bottom: 0.5rem; 
    }
    .content p { 
        margin-bottom: 1rem; 
    }
    .content ul { 
        list-style-type: disc; 
        list-style-position: inside; 
        margin-bottom: 1rem; 
    }
    .content ol { 
        list-style-type: decimal; 
        list-style-position: inside; 
        margin-bottom: 1rem; 
    }
    .content a { 
        color: #2563eb; 
    }
    .content a:hover { 
        color: #1e40af; 
    }
    .content blockquote { 
        border-left: 4px solid #d1d5db; 
        padding-left: 1rem; 
        font-style: italic; 
    }

    
.content table {
    width: 100%;
    border-collapse: collapse;
    margin: 1.5rem 0;
    font-size: 0.875rem;
    box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1), 0 1px 2px 0 rgba(0, 0, 0, 0.06);
    border-radius: 0.5rem;
    overflow: hidden;
    border: 1px solid #e5e7eb;
}

.dark-mode .content table {
    border: 1px solid #4b5563;
}

.content thead {
    background-color: #eaebec;
}

.dark-mode .content thead {
    background-color: #374151;
}

.content th {
    padding: 0.75rem 1rem;
    text-align: left;
    font-weight: 600;
    color: #374151;
    border-bottom: 1px solid #e5e7eb;
    border-right: 1px solid #e5e7eb;  
    font-size: 0.875rem;
    text-transform: uppercase;
    letter-spacing: 0.05em;
}

.dark-mode .content th {
    color: #f9fafb;
    border-bottom: 1px solid #4b5563;
    border-right: 1px solid #4b5563;  
}

.content th:last-child {
    border-right: none;  
}

.content td {
    padding: 0.75rem 1rem;
    border-bottom: 1px solid #e5e7eb;
    border-right: 1px solid #e5e7eb;  
    color: #6b7280;
}

.dark-mode .content td {
    color: #d1d5db;
    border-bottom: 1px solid #4b5563;
    border-right: 1px solid #4b5563;  
}

.content td:last-child {
    border-right: none;  
}

.content tbody tr {
    transition: background-color 0.15s ease-in-out;
}

.content tbody tr:hover {
    background-color: #f9fafb;
}

.dark-mode .content tbody tr:hover {
    background-color: #374151;
}

.content tbody tr:last-child td {
    border-bottom: none;
}

 
.content tbody tr:nth-child(even) {
    background-color: #f4f5f7;
}

.dark-mode .content tbody tr:nth-child(even) {
    background-color: #1f2937;
}

.dark-mode .content tbody tr:nth-child(even):hover {
    background-color: #374151;
}

 
.content code:not(pre code) {
    background-color: #f3f4f6;
    color: #dc2626;
    padding: 0.125rem 0.375rem;
    border-radius: 0.25rem;
    font-size: 0.875em;
    font-weight: 500;
    border: 1px solid #e5e7eb;
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', 'Consolas', 'source-code-pro', monospace;
}

.dark-mode .content code:not(pre code) {
    background-color: #224464;
    color: #f87171;
    border: 1px solid #4b5563;
}

 
.content pre {
    background-color: #224464;
    color: #f9fafb;
    padding: 1rem;
    border-radius: 0.5rem;
    overflow-x: auto;
    margin: 1.5rem 0;
    border: 1px solid #374151;
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', 'Consolas', 'source-code-pro', monospace;
    font-size: 0.875rem;
    line-height: 1.5;
}

.dark-mode .content pre {
    background-color: #111827;
    border: 1px solid #374151;
}

 
.content pre code {
    background: none;
    color: inherit;
    padding: 0;
    border: none;
    font-size: inherit;
    font-weight: normal;
}

 
.content pre code .keyword { color: #f472b6; }  
.content pre code .function { color: #60a5fa; }  
.content pre code .string { color: #34d399; }  
.content pre code .comment { color: #9ca3af; font-style: italic; }  
.content pre code .number { color: #fbbf24; }  
.content pre code .class { color: #c084fc; }  
.content pre code .operator { color: #93c5fd; }  

 
.content pre::-webkit-scrollbar {
    height: 6px;
}

.content pre::-webkit-scrollbar-track {
    background: #374151;
    border-radius: 0 0 0.5rem 0.5rem;
}

.content pre::-webkit-scrollbar-thumb {
    background: #6b7280;
    border-radius: 3px;
}

.content pre::-webkit-scrollbar-thumb:hover {
    background: #9ca3af;
}

 
.content h1 code:not(pre code),
.content h2 code:not(pre code),
.content h3 code:not(pre code) {
    font-size: 0.9em;
    background-color: #fef2f2;
    color: #dc2626;
}

.dark-mode .content h1 code:not(pre code),
.dark-mode .content h2 code:not(pre code),
.dark-mode .content h3 code:not(pre code) {
    background-color: #7f1d1d;
    color: #fca5a5;
}

 
.content a code:not(pre code) {
    color: inherit;
    background-color: rgba(59, 130, 246, 0.1);
}

.dark-mode .content a code:not(pre code) {
    background-color: rgba(96, 165, 250, 0.2);
}
        :root {
            --color-bg: #1a1a1a;
            --color-text: #e6e6e6;
            --color-accent: #6b7280;
            --color-link: #9ca3af;
            --color-link-hover: #d1d5db;
        }
        
        .light-mode {
            --color-bg: #f9fafb;
            --color-text: #1f2937;
            --color-accent: #6b7280;
            --color-link: #4b5563;
            --color-link-hover: #111827;
        }
        
        body {
            background-color: var(--color-bg);
            color: var(--color-text);
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif;
            transition: background-color 0.3s, color 0.3s;
        }
        
        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }
        
        a:hover {
            color: var(--color-link-hover);
        }
        
        .sidebar {
            position: fixed;
             
            height: 100vh;
            overflow-y: auto;
            
        }
        
        .main-content {
            margin-left: 350px;
            
             
        }
        
        .toc-container {
            position: fixed;
            right: 2rem;
            top: 2rem;
            width: 250px;
            display: none;
        }
        
        @media (min-width: 1280px) {
            .toc-container {
                display: block;
            }
        }
        
        @media (max-width: 768px) {
            .sidebar {
                transform: translateX(-100%);
                transition: transform 0.3s ease;
                z-index: 50;
            }
            
            .sidebar.open {
                transform: translateX(0);
            }
            
            .main-content {
                margin-left: 0;
                 
            }
            
            .mobile-menu-btn {
                display: block;
                position: fixed;
                top: 1rem;
                left: 1rem;
                z-index: 100;
            }
        }
        
        .toc-link.active {
            color: var(--color-link-hover);
            font-weight: bold;
        }
        
        .social-icons a {
            margin-right: 0.5rem;
        }
        
        pre {
            background-color: rgba(107, 114, 128, 0.1);
            padding: 1rem;
            border-radius: 0.25rem;
            overflow-x: auto;
        }
        
        code {
            font-family: Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }
    </style>
    <script src="https://cdn.tailwindcss.com"></script>
   <script>
      tailwind.config = {
        theme: {
          extend: {
            colors: {
              brand: {
                50: '#eff6ff',
                100: '#dbeafe',
                500: '#0ea5e9',
                600: '#0284c7',
              },
            },
            boxShadow: {
              soft: '0 8px 24px rgba(2, 132, 199, 0.08)'
            }
          },
        },
      }
    </script>
</head>
  <body class="light-mode">
      <header>
    
  </header>
    
    <button class="mobile-menu-btn p-2 bg-gray-800 text-white rounded md:hidden">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
        </svg>
    </button>

    
    <aside class="sidebar border-r-4 border-sky-800  min-w-[350px] py-12 px-10 bg-gradient-to-br from-cyan-200 via-green-100 to-sky-100">
        <div class="flex flex-col items-center mb-8">
                
            
            <div class="w-24 h-24 mb-4 relative">
                <a href="/">
                <svg version="1.0" xmlns="http://www.w3.org/2000/svg"
 viewBox="0 0 550.000000 385.000000"
 preserveAspectRatio="xMidYMid meet">

<g transform="translate(0.000000,385.000000) scale(0.100000,-0.100000)"
fill="#000000" stroke="none">
<path d="M2780 3000 l0 -300 40 0 40 0 0 260 0 260 1165 0 1165 0 0 -1245 0
-1245 -1165 0 -1165 0 0 260 0 260 -40 0 -40 0 0 -300 0 -300 1245 0 1245 0 0
1325 0 1325 -1245 0 -1245 0 0 -300z"/>
<path d="M700 1975 l0 -635 145 0 145 0 0 245 0 245 383 0 c422 0 426 1 491
66 60 60 66 88 66 319 0 190 -2 214 -21 255 -25 56 -53 86 -104 113 -39 22
-49 22 -572 25 l-533 3 0 -636z m940 245 l0 -100 -325 0 -325 0 0 100 0 100
325 0 325 0 0 -100z"/>
<path d="M2320 2601 c-69 -22 -112 -58 -146 -121 -17 -31 -19 -60 -19 -260 0
-211 1 -227 22 -265 27 -51 68 -90 115 -109 32 -14 98 -16 438 -16 l400 0 0
-245 0 -245 145 0 145 0 0 536 c0 373 -3 548 -11 575 -16 50 -61 105 -112 132
-41 22 -48 22 -497 24 -250 1 -466 -2 -480 -6z m810 -381 l0 -100 -340 0 -340
0 0 100 0 100 340 0 340 0 0 -100z"/>
<path d="M3650 2607 c0 -1 99 -144 219 -317 l219 -314 -63 -91 c-34 -49 -133
-191 -219 -315 l-157 -225 172 -3 c97 -1 176 2 183 7 6 5 72 97 146 205 74
107 138 196 141 196 4 0 71 -92 149 -205 l143 -205 173 0 c96 0 174 2 174 4 0
3 -95 141 -211 308 -116 167 -214 308 -216 315 -3 7 92 151 211 322 119 170
216 312 216 315 0 3 -78 6 -174 6 l-174 0 -143 -205 c-78 -113 -145 -205 -149
-205 -3 0 -70 92 -148 205 l-143 205 -174 0 c-96 0 -175 -1 -175 -3z"/>
</g>
</svg>
</a>
            </div>

            <p class="text-xs text-center text-gray-500 max-w-[200px]">
                Профессиональное агентство контекстной рекламы с опытом более 18 лет.

            </p>
        </div>

        
        <nav class="mb-8">
          
        
            
  <nav>
    <ul>
    <li class="text-lg font-semibold">
      <a href="/">Контекстная реклама</a>
    </li>
    <li class="text-lg font-semibold">
      <a href="/analytics/">web-аналитика</a>
    </li>
    <li class="text-lg font-semibold">
      <a aria-current="true" class="ancestor" href="/posts/">Блог</a>
    </li>
    <li class="text-lg font-semibold">
      <a href="/about/">О нас</a>
    </li>
    <li class="text-lg font-semibold">
      <a href="/tags/">Категории</a>
    </li>
    <li class="text-lg font-semibold">
      <a href="/job/">Вакансии</a>
    </li>
    <li class="text-lg font-semibold">
      <a href="/tags/case/">Кейсы</a>
    </li>
    <li class="text-lg font-semibold">
      <a href="/calculators/">Калькуляторы</a>
    </li>
    <li class="text-lg font-semibold">
      <a href="/contacts/">Контакты</a>
    </li>
    </ul>
  </nav>

           

        </nav>

        
        <div class="social-icons  mt-auto">
<div>

<a href="https://t.me/your_link" target="_blank" class="inline-flex 
items-center justify-center px-3 py-3 text-white bg-[#2AABEE] font-medium text-sm rounded 
shadow-md hover:bg-[#279fd9] 
hover:shadow-lg focus:bg-[#2493c8] 
focus:shadow-lg focus:outline-none 
focus:ring-0 active:bg-[#2185b3] active:shadow-lg transition duration-150 ease-in-out">
  
     
<svg width="25px" height="25px" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" 
fill="#fff" class="bi bi-telegram pr-1">
  <path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0zM8.287 5.906c-.778.324-2.334.994-4.666 2.01-.378.15-.577.298-.595.442-.03.243.275.339.69.47l.175.055c.408.133.958.288 1.243.294.26.006.549-.1.868-.32 2.179-1.471 3.304-2.214 3.374-2.23.05-.012.12-.026.166.016.047.041.042.12.037.141-.03.129-1.227 1.241-1.846 1.817-.193.18-.33.307-.358.336a8.154 8.154 0 0 1-.188.186c-.38.366-.664.64.015 1.088.327.216.589.393.85.571.284.194.568.387.936.629.093.06.183.125.27.187.331.236.63.448.997.414.214-.02.435-.22.547-.82.265-1.417.786-4.486.906-5.751a1.426 1.426 0 0 0-.013-.315.337.337 0 0 0-.114-.217.526.526 0 0 0-.31-.093c-.3.005-.763.166-2.984 1.09z"/>
</svg>
                            
  
  Открыть в Telegram
</a>

</div>
            <div class="pt-6 flex gap-1 justify-left items-center">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />
                </svg>
                sale@p9x.ru
            </div>
        </div>
        
        
        <div class="mt-8 pt-4 text-xs text-gray-500 border-t border-gray-700">
            <p>Powered by <a href="#" class="hover:underline">Hugo</a> | Themed with <a href="#" class="hover:underline">poison</a></p>
            <p>© 2010 - 2025 Poison. All rights reserved.</p>
        </div>
    </aside>

  <main class="main-content">
 
            
            
   <div class="max-w-[1100px]">
    
    <section class="bg-white border border-gray-200  shadow-sm overflow-hidden">
      
      <div class="bg-gradient-to-b from-sky-50/60 to-transparent p-6 border-b border-gray-200">
        <h1 class="text-2xl md:text-3xl font-bold">Руководство по обучению с подкреплением: Руководство по основополагающим вопросам</h1>
        <time class="block mt-2 text-gray-500" datetime="2025-11-19T23:13:09+00:00">
          
    
    November 19, 2025
        </time>

              
        <div class="not-prose backdrop-blur-sm pt-4">
          <div class="flex flex-col gap-4 sm:flex-row sm:items-center sm:justify-between">
            <div class="flex items-center gap-3">
              
                <img src="/user_02.png" class="h-20 w-20 rounded-full hue-rotate-270" alt="">
              
              <div>
                <p class="text-gray-800 font-medium">Автор: Дмитрий Иванов [Команда P9X]</p>
                <p class="text-gray-600 text-sm">~8 минут чтения</p>
              </div>
            </div>
            <div class="text-sm text-gray-600">
                <div class="tags mt-8">
      

    </div>
            </div>
          </div>
        </div>

      </div>

      
      <article class="px-6 py-6 max-w-[94ch] content">
  

        


        
     
           <h3 id="основы-обучения-с-подкреплением">Основы обучения с подкреплением</h3>
<p>Основные понятия, которые нужно знать, чтобы понять обучение с подкреплением (Reinforcement Learning)!</p>
<p>Мы начнём с самых основ — «что такое RL?» — и перейдём к более сложным темам, включая исследование среды, значения и политики агентов, а также различия между популярными подходами к обучению. По пути мы также узнаем о различных проблемах в RL и о том, как исследователи их решали.</p>
<p>В конце статьи я также поделюсь видео на YouTube, в котором объясняются все понятия из этой статьи в наглядной форме. Если вы не очень любите читать, вы можете посмотреть это видео вместо этого!</p>
<h4 id="основы-обучения-с-подкреплением-1">Основы обучения с подкреплением</h4>
<p>Представьте, что вы хотите обучить модель искусственного интеллекта (ИИ) ориентироваться в полосе препятствий. RL — это раздел машинного обучения, в котором наши модели учатся, собирая опыт — предпринимая действия и наблюдая за происходящим.</p>
<p>Более формально RL состоит из двух компонентов — агента и среды.</p>
<p><strong>Агент</strong></p>
<p>Процесс обучения включает в себя две ключевые активности, которые повторяются снова и снова: <strong>исследование</strong> и <strong>обучение</strong>. Во время исследования агент собирает опыт в среде, предпринимая действия и выясняя, что происходит. А затем, во время обучения, агент использует этот опыт для улучшения себя.</p>
<p>Агент собирает опыт в среде и использует его для обучения политике.</p>
<p><strong>Среда</strong></p>
<p>Как только агент выбирает действие, среда обновляется. Она также возвращает вознаграждение в зависимости от того, насколько хорошо агент справляется. Разработчик среды программирует структуру вознаграждения.</p>
<p>Например, предположим, вы работаете над средой, которая обучает ИИ избегать препятствий и достигать цели. Вы можете запрограммировать свою среду так, чтобы она возвращала положительное вознаграждение, когда агент приближается к цели. Но если агент сталкивается с препятствием, вы можете запрограммировать его на получение большого отрицательного вознаграждения.</p>
<p>Другими словами, среда обеспечивает <em>положительное подкрепление</em> (например, высокое положительное вознаграждение), когда агент делает что-то <em>хорошее</em>, и <em>наказание</em> (например, отрицательное вознаграждение), когда он делает что-то <em>плохое</em>.</p>
<p>Хотя агент не знает, как на самом деле работает среда, он всё равно может определить из своих моделей вознаграждения, как выбрать оптимальные действия, которые приведут к максимальным вознаграждениям.</p>
<p><strong>Политика</strong></p>
<p>На каждом шаге ИИ-агент наблюдает текущее состояние среды и выбирает действие. Цель RL — научиться сопоставлять наблюдения с действиями, то есть «при данном состоянии, которое я наблюдаю, какое действие мне выбрать?».</p>
<p>В терминах RL это сопоставление состояния с действием также называется <strong>политикой</strong>.</p>
<p>Эта политика определяет, как агент ведёт себя в разных состояниях, и в <em>глубоком</em> обучении с подкреплением мы обучаем эту функцию, обучая некоторую <em>глубокую</em> нейронную сеть.</p>
<h4 id="обучение-с-подкреплением">Обучение с подкреплением</h4>
<p>Агент наблюдает состояние S, запрашивает сеть для генерации действия A. Среда выполняет действие и возвращает вознаграждение r и следующее состояние s’. Это продолжается до тех пор, пока эпизод не завершится. Каждый шаг, который делает агент, позже используется для обучения сети политики агента.</p>
<p>Понимание различий и взаимодействия между агентом, политикой и средой очень важно для понимания обучения с подкреплением.</p>
<ol>
<li>Агент — это обучающийся, который исследует и предпринимает действия в среде.</li>
<li>Политика — это стратегия (часто нейронная сеть), которую агент использует для определения того, какое действие предпринять в данном состоянии. В RL наша конечная цель — обучить эту стратегию.</li>
<li>Среда — это внешняя система, с которой взаимодействует агент, которая обеспечивает обратную связь в виде вознаграждений и новых состояний.</li>
</ol>
<p>Вот краткое определение, которое вы должны запомнить:</p>
<blockquote>
<p>В обучении с подкреплением агент следует политике, чтобы выбрать действия в среде.</p>
</blockquote>
<h4 id="наблюдения-и-действия">Наблюдения и действия</h4>
<p>Агент исследует среду, предпринимая последовательность «шагов». Каждый шаг — это одно решение. Агент <em>наблюдает</em> состояние среды. Решает о <em>действии</em>. Получает <em>вознаграждение</em>. Наблюдает <em>следующее состояние</em>. В этом разделе давайте разберёмся, что такое наблюдения и действия.</p>
<p><strong>Наблюдение</strong></p>
<p>Наблюдение — это то, что агент видит в среде — информация, которую он получает о текущем состоянии среды. В среде навигации по препятствиям наблюдение может быть проекциями LiDAR для обнаружения препятствий. Для игр Atari это может быть история последних нескольких пиксельных кадров. Для генерации текста это может быть контекст сгенерированных токенов. В шахматах это положение всех фигур, чей ход.</p>
<p>Наблюдение в идеале содержит всю информацию, необходимую агенту для совершения действия.</p>
<p><strong>Действие</strong></p>
<p>Пространство действий — это все доступные решения, которые может принять агент. Действия могут быть дискретными или непрерывными. Дискретное пространство действий — это когда агент должен выбирать между определённым набором категориальных решений. Например, в играх Atari действия могут быть кнопками контроллера Atari. Для генерации текста это выбор между всеми токенами, присутствующими в словаре модели. В шахматах это может быть список доступных ходов.</p>
<h4 id="важный-урок">Важный урок</h4>
<p>Но вот что важно понять: для агента и политики — среда и её особенности могут быть чёрным ящиком. Агент будет получать информацию о состоянии в виде вектора, генерировать действие, получать вознаграждение и позже учиться на этом.</p>
<p>Так что в своём уме вы можете считать агента и среду двумя отдельными сущностями. Среда определяет пространство состояний, пространство действий, стратегии вознаграждения и правила.</p>
<p>Эти правила не зависят от того, как агент исследует и как политика обучается на собранном опыте.</p>
<p>При изучении исследовательской статьи важно уточнить в уме, о каком аспекте RL мы читаем. Это новая среда? Это новый метод обучения политики? Это стратегия исследования? В зависимости от ответа вы можете рассматривать другие вещи как чёрный ящик.</p>
<h4 id="исследование">Исследование</h4>
<p>Как агент исследует и собирает опыт?</p>
<p>Каждый алгоритм RL должен решить одну из крупнейших дилемм при обучении агентов RL — исследование против эксплуатации.</p>
<p><strong>Исследование означает опробование новых действий для сбора информации об среде.</strong> Представьте, что вы учитесь сражаться с боссом в сложной видеоигре. Сначала вы будете пробовать разные подходы, разное оружие, заклинания, случайные вещи, чтобы увидеть, что работает, а что нет.</p>
<p>Однако, как только вы начнёте получать вознаграждения, например, постоянно наносить урон боссу, вы перестанете исследовать и начнёте использовать уже приобретённую стратегию. <strong>Эксплуатация означает жадное выбор действий, которые, как вы думаете, принесут наилучшие вознаграждения.</strong></p>
<blockquote>
<p>Хорошая стратегия исследования в RL должна сбалансировать исследование и эксплуатацию.</p>
</blockquote>
<p>Популярная стратегия исследования — <strong>эпсилон-жадная</strong>, где агент исследует со случайным действием часть времени (определяется параметром эпсилон), а использует своё наилучшее известное действие в остальное время. Это значение эпсилон обычно велико в начале и постепенно уменьшается, чтобы отдавать предпочтение эксплуатации по мере обучения агента.</p>
<p>Эпсилон-жадный — это метод исследования, при котором RL-агент выбирает случайное действие время от времени.</p>
<p>Эпсилон-жадный работает только в дискретных пространствах действий. В непрерывных пространствах исследование часто обрабатывается двумя популярными способами. Один из способов — добавить немного случайного шума к действию, которое агент решает предпринять. Другой популярный метод — добавить <strong>бонус энтропии</strong> в функцию потерь, который побуждает политику быть менее уверенной в своём выборе, что естественным образом приводит к более разнообразным действиям и исследованию.</p>
<p>Некоторые другие способы поощрения исследования:</p>
<ul>
<li>Разработка среды для использования случайной инициализации состояний в начале эпизодов.</li>
<li>Внутренние методы исследования, где агент действует из собственного «любопытства». Алгоритмы вроде Curiosity и RND вознаграждают агента за посещение новых состояний или за выполнение действий, результат которых трудно предсказать.</li>
</ul>
<p>Я расскажу об этих увлекательных методах в своём видео об агенте, так что обязательно посмотрите его!</p>
<h4 id="алгоритмы-обучения">Алгоритмы обучения</h4>
<p>Большинство исследовательских работ и академических тем в области обучения с подкреплением посвящены оптимизации стратегии агента для выбора действий. Цель алгоритмов оптимизации — научиться действиям, которые максимизируют долгосрочные ожидаемые вознаграждения.</p>
<p>Давайте рассмотрим различные алгоритмические варианты один за другим.</p>
<h4 id="модельное-и-модельное-свободное-обучение">Модельное и модельное свободное обучение</h4>
<p>Итак, наш агент исследовал среду и собрал массу опыта. Теперь что?</p>
<p>Учится ли агент действовать непосредственно из этого опыта? Или сначала он пытается смоделировать динамику и физику среды?</p>
<p>Один подход — <strong>модель-ориентированное обучение</strong>. Здесь агент сначала использует свой опыт для создания собственной внутренней симуляции или модели мира. Эта модель учится предсказывать последствия своих действий, то есть, учитывая состояние и действие, каково будет следующее состояние и вознаграждение? Как только у него появится эта модель, он может практиковаться и планировать полностью в пределах своего воображения, запуская тысячи симуляций, чтобы найти лучшую стратегию, не делая рискованных шагов в реальном мире.</p>
<p>Модель-ориентированное RL изучает отдельную модель, чтобы узнать, как работает среда.</p>
<p>Это особенно полезно в средах, где сбор опыта в реальном мире может быть дорогостоящим — например, в робототехнике или беспилотных автомобилях. Примеры модель-ориентированного RL: Dyna-Q, World Models, Dreamer и т. д. Я напишу отдельную статью, чтобы рассмотреть эти модели более подробно.</p>
<p>Второй подход называется <strong>модель-свободным обучением</strong>. Здесь агент рассматривает среду как чёрный ящик и изучает политику непосредственно на основе собранного опыта. Давайте поговорим о модель-свободном RL в следующем разделе.</p>
<h4 id="обучение-основанное-на-ценности">Обучение, основанное на ценности</h4>
<p>Существуют два основных подхода к модель-свободным алгоритмам RL.</p>
<p>Алгоритмы, основанные на ценности, учатся оценивать, насколько хорошо каждое состояние. Алгоритмы, основанные на политике, учатся напрямую, как действовать в каждом состоянии.</p>
<p><strong>Q-Learning</strong></p>
<p>Мы узнали об интуиции, стоящей за значениями состояний, но как мы используем эту информацию для изучения действий? Уравнение Q-Learning отвечает на этот вопрос.</p>
<p>Q(s, a) = r + γ * max_a Q(s’, a’)</p>
<p>Значение Q(s,a) — это _q_uality-value _действия a в состоянии s. Приведённое выше уравнение по сути гласит: качество действия a в состоянии s равно немедленному вознаграждению <strong>r</strong>, которое вы получаете, находясь в состоянии s, плюс дисконтированное качество значения следующего наилучшего действия.</p>
<h4 id="td-learning-vs-mc-sampling">TD Learning vs MC Sampling</h4>
<p>Как агент объединяет будущий опыт, чтобы научиться?</p>
<p>В обучении с временными различиями (TD Learning) агент обновляет свои оценки значений после каждого шага, используя уравнение Беллмана. И он делает это, видя свою собственную оценку Q-значения в следующем состоянии. Эта стратегия называется 1-шаговым TD Learning или одношаговым обучением с временными различиями. Вы делаете один шаг и обновляете своё обучение на основе своих прошлых оценок.</p>
<p>Второй вариант называется Монте-Карло выборкой. Здесь агент ждёт, пока закончится весь эпизод, прежде чем что-либо обновлять. И затем он использует полный возврат из эпизода:</p>
<p>Q(s,a) = r₁ + γr₂ + γ²r₃ + … + γⁿrₙ</p>
<p>В MC Sampling мы завершаем весь эпизод, чтобы вычислить оценки на основе фактических вознаграждений.</p>
<h4 id="градиенты-политики">Градиенты политики</h4>
<p>Теперь, когда мы поняли концепцию TD-Learning vs MC Sampling, пришло время вернуться к методам обучения, основанным на политике.</p>
<p>Напомним, что методы, основанные на ценности, такие как DQN, сначала должны явно рассчитать значение или Q-значение для каждого возможного действия, а затем выбрать лучшее. Но можно пропустить этот шаг, и методы градиента политики, такие как <strong>REINFORCE</strong>, делают именно это.</p>
<p>В REINFORCE сеть политики выводит вероятности для каждого действия, и мы обучаем её увеличивать вероятность действий, которые приводят к хорошим результатам. Для дискретных пространств методы PG выводят вероятность каждого действия в виде категориального распределения. Для непрерывных пространств методы PG выводят как гауссовские распределения, прогнозируя среднее значение и стандартное отклонение каждого элемента в векторе действий.</p>
<h4 id="advantage-actor-critic">Advantage Actor Critic</h4>
<p>Если вы попытаетесь использовать vanilla REINFORCE на большинстве сложных задач, он будет бороться, и причина этого двояка.</p>
<p>Первая — потому что он страдает от высокой дисперсии, потому что это метод Монте-Карло. Вторая — у него нет чувства базовой линии. Например, представьте среду, которая всегда даёт вам положительное вознаграждение, тогда возвраты никогда не будут отрицательными, так что REINFORCE будет увеличивать вероятности всех действий, хотя и непропорционально.</p>
<p>Мы не хотим вознаграждать действия только за то, что они получили положительный балл. Мы хотим вознаграждать их за то, что они <strong>лучше среднего</strong>.</p>
<p>И вот где концепция <em>преимущества</em> становится важной. Вместо того чтобы просто использовать необработанный возврат для обновления нашей политики, мы вычтем ожидаемый возврат для этого состояния. Итак, наш новый сигнал обновления становится:</p>
<p><strong>Advantage = Возврат, который вы получили – Возврат, который вы ожидали</strong></p>
<p>В то время как Advantage даёт нам базовую линию для наших наблюдаемых возвратов, давайте также обсудим концепцию методов Actor Critic.</p>
<p><strong>Actor Critic</strong> сочетает в себе лучшее из методов, основанных на ценности (таких как DQN), и лучшее из методов, основанных на политике (таких как REINFORCE). Методы Actor Critic обучают отдельную нейронную сеть «критика», которая обучена только оценивать состояния, подобно Q-сети, описанной ранее.</p>
<p>Метод актёра, с другой стороны, изучает политику.</p>
<h4 id="заключение">Заключение</h4>
<p>Эта статья является дополнением к видео на YouTube ниже, которое я сделал. Не стесняйтесь проверить это, если вам понравилось это чтение.</p>
<p>Каждый алгоритм делает определённый выбор для каждого вопроса, и эти выборы каскадом проходят через всю систему, влияя на всё, от эффективности выборки до стабильности и производительности в реальных условиях.</p>
<p>В конце концов, создание алгоритма RL — это ответ на эти проблемы путём принятия вами определённых решений. DQNs выбирают изучение ценностей. методы политики напрямую изучают <strong>политику</strong>. Методы Монте-Карло обновляют после полного эпизода, используя фактические возвраты — это делает их беспристрастными, но у них высокая дисперсия из-за стохастической природы исследования RL. TD Learning вместо этого выбирает обучение на каждом шаге, основанном на собственных оценках агента. Методы Actor Critic сочетают в себе лучшее из методов, основанных на ценности (таких как DQN), и лучшее из методов, основанных на политике (таких как REINFORCE).</p>

        

      </article>
    </section>
  </div>



          
<footer class="px-6 py-12 bg-sky-950 text-white/50">
     <div class="inset-0 bg-gradient-to-br from-slate-900 via-slate-800 to-slate-900"></div>
      <div class="relative">
 
          <div class="border-t border-white/10"></div>

          
          <div class="mt-6 flex flex-col md:flex-row items-center justify-between gap-4">
            <p class="text-sm text-slate-400">© <span id="year"></span>P9X.ru 2010 - 2025г. услуги интернет маркетинга</p>
            <div class="flex items-center gap-4 text-slate-400 text-sm">
              <a href="#" class="hover:text-white transition">Документы</a>
              <a href="#" class="hover:text-white transition">Служба поддержки</a>
              <a href="#" class="hover:text-white transition">Статус</a>
            </div>
          </div>      
      </div>


<script type="text/javascript">
    (function(m,e,t,r,i,k,a){
        m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
        m[i].l=1*new Date();
        for (var j = 0; j < document.scripts.length; j++) {if (document.scripts[j].src === r) { return; }}
        k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)
    })(window, document,'script','https://mc.yandex.ru/metrika/tag.js?id=105149615', 'ym');

    ym(105149615, 'init', {ssr:true, webvisor:true, clickmap:true, ecommerce:"dataLayer", accurateTrackBounce:true, trackLinks:true});
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/105149615" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

  </footer>
  </main>          

    <script>

        
        
        const mobileMenuBtn = document.querySelector('.mobile-menu-btn');
        const sidebar = document.querySelector('.sidebar');
        
        mobileMenuBtn.addEventListener('click', () => {
            sidebar.classList.toggle('open');
        });
        
        
        const observerOptions = {
            root: null,
            rootMargin: '0px',
            threshold: 0.5
        };
        
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                const id = entry.target.getAttribute('id');
                if (entry.isIntersecting) {
                    document.querySelectorAll(`.toc-link[href="#${id}"]`).forEach(link => {
                        link.classList.add('active');
                    });
                } else {
                    document.querySelectorAll(`.toc-link[href="#${id}"]`).forEach(link => {
                        link.classList.remove('active');
                    });
                }
            });
        }, observerOptions);
        
        
        document.querySelectorAll('h2[id]').forEach(heading => {
            observer.observe(heading);
        });
    </script>



</body>
</html>
